{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice problems - midterm 1\n",
    "\n",
    "Exam 1 will likely consist of problems that may be like the sample ones below in spirit, the problems in the psets, and material in the lectures.  The number of problems will of course be fewer for a 50 minute exam.  Also familiarizing yourselves with these sort of problems will, no doubt, make the exam itself easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1.\n",
    "\n",
    "a)  The rowspace of $\\begin{pmatrix} 1 & -1 & 0 \\\\ 0 & 1 & -1 \\\\ 0 & 0 & 0 \\end{pmatrix}$\n",
    "may be described as a plane through the origin.  What is the normal to this plane?\n",
    "<br>\n",
    "(This problem can be done by inspection and common sense.)\n",
    "<br> b) Also describe the nullspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "a) The row space of this matrix is the set of all vectors that can be written as a linear combination of its rows. In this case, the row space is the set of all vectors of the form\n",
    "$$\\begin{pmatrix} x \\\\ y \\\\ z \\end{pmatrix} = a\\begin{pmatrix} 1 \\\\ -1 \\\\ 0 \\end{pmatrix} + b \\begin{pmatrix} 0 \\\\ 1 \\\\ -1 \\end{pmatrix}$$\n",
    "for scalars $a,b\\in\\mathbb{R}$. This gives three equations $x=a, y=b-a$ and $z=-b$ in terms of the two scalars. We can eliminate these two scalars and write a relationship between the three components:\n",
    "$$x+y+z = 0.$$\n",
    "We notice that this is the equation of a plane through the origin, with the normal $\\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}$.\n",
    "\n",
    "b) The nullspace is the set of all vectors $v = \\begin{pmatrix} v_1 \\\\ v_2 \\\\ v_3 \\end{pmatrix} $ for which\n",
    "$$\\begin{pmatrix} 1 & -1 & 0 \\\\ 0 & 1 & -1 \\\\ 0 & 0 & 0 \\end{pmatrix}v = 0$$\n",
    "This matrix equation implies that $v_1=v_2=v_3$, so the nullspace of the matrix is the set of all vectors of the form\n",
    "$$c\\begin{pmatrix} 1 \\\\ 1 \\\\ 1\\end{pmatrix}$$ \n",
    "for any real number $c$. This is a line in $\\mathbb{R}^3$, and in fact is parallel to the normal we found in part a). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2.\n",
    "\n",
    "a) Suppose $A=LU$ where $A,L,$ and $U$ are nxn invertible.  Write the solution to $Ax=b$ in terms of $b$ and possibly\n",
    "$L$,$U$,$L^{-1}$ and $U^{-1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Suppose $A = QR$ is square,  where $Q$ is orthogonal and $R$ is upper triangular and invertible.  Write the solution to $Ax=b$ in terms of $b$ and possibly\n",
    "$Q$,$Q^T$,and $R$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Suppose $A=U\\Sigma V^T$ is the rank $r$ svd for a rectangular $A$ and $b=Uw$.  Write down a solution to $Ax=b$ \n",
    "in terms of $b$ and possibly $U$,$U^T$,$\\Sigma$,$\\Sigma^{-1}$, $V$, or $V^T$ and $w$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "a) If $Ax = b$, then $LUx = b$. Since $L$ and $U$ are invertible, we can write the solution as \n",
    "$$\\boxed{x = (LU)^{-1}b = U^{-1}L^{-1} b}$$\n",
    "b) If $Ax = b$, then $QRx = b$. $R$ is invertible and $Q$ is (square) orthogonal so that $Q^{-1}=Q^T$. We can therefore write the solution as \n",
    "$$\\boxed{x = (QR)^{-1}b = R^{-1}Q^{-1}b = R^{-1}Q^T b}$$\n",
    "c) If $Ax = b$, then $U\\Sigma V^Tx = Uw$. We can multiply both sides by $U^T$ to obtain $\\Sigma V^T x = w$. We can then multiply both sides by $\\Sigma^{-1}$ to obtain $V^Tx = \\Sigma^{-1}w$. Since $A$ is rectangular matrix, $V$ is not necessarily square, and so may not be invertible. However, we know that $b$ is in the column space of $U$, and therefore in the column space of $A$, so this system definitely has a solution. By inspection, we can see that a possible solution is $\\boxed{x=V\\Sigma^{-1} w}$, since $V^T(V\\Sigma^{-1} w)=\\Sigma^{-1}w$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3.\n",
    "\n",
    "a) How many parameters are needed to represent the  $n \\times n$ identity matrix on a computer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) How many parameters are needed to represent the matrix $vv^T+$Diagonal($v$) on a computer, where $v \\in \\mathbb{R}^n$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) How many parameters are needed to represent the $\\Sigma$ in the rank-r svd?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) How many parameters are needed to represent the $L$ in a unit $n$ by $n$ lower triangular matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution \n",
    "\n",
    "a) Only one parameter is needed to represent the $n\\times n$ identity matrix. It only has ones on the diagonal. The only piece of information we need to store is the dimensions of the matrix. But since the identity matrix is always square, this can be stored in a single parameter.\n",
    "\n",
    "b) A matrix of this form is uniquely determined by the vector $v\\in \\mathbb{R}^n$. It therefore depends on $n$ parameters (the components of $v$).\n",
    "\n",
    "c) In general, $r$ parameters are needed to represent the $\\Sigma$ in the SVD. It is a diagonal matrix, so we only need to store the $r$ singular values, which lie along its diagonal.\n",
    "\n",
    "d) The $L$ in the $LU$ factorization is a lower triangular matrix. Every element along the diagonal is equal to $1$. We therefore only need to store the elements in the matrix that lie below the main diagonal. A $n\\times n$ matrix has $n^2$ components, there are $n$ elements on the diagonal, and therefore there are\n",
    "$$\\boxed{\\frac{n^2-n}{2}}$$\n",
    "components in the matrix below the main diagonal, i.e. there are $(n^2-n)/2$ parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4.\n",
    "\n",
    "Am I necessarily a vector space? Why or why not?  (Argue that all real linear combinations are in the space, or zero or some other linear combination is not in the space.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) In $R^3$ the xy-plane together with the z axis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) All differentiable functions $y(x)$ that satisfy $y'(x)=y(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Given any two matrices $A$ and $B$, all vectors $x$ such that $Ax=0$ and $Bx=0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Given any two matrix $A$ and $B$, all vectors $x$ such that either $Ax=0$ or $Bx=0$ (or both)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) All polynomials in $x$ whose value at $x=2019$ is $0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f) All 10x10 matrices whose diagonal elements add to $0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution \n",
    "\n",
    "a) This is **not** a vector space. Consider the vectors $v_1 = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}$ and  $v_2 = \\begin{pmatrix} 0\\\\ 0 \\\\ 1 \\end{pmatrix}$. Then $v_1$ lies in the $xy$-plane, and $v_2$ is on the $z$-axis. However, their sum $v_1+v_2 = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}$ which is neither in the $xy$-plane noe on the $z$-axis.\n",
    "\n",
    "b) This is a vector space. Take two functions $y_1(x)$ and $y_2(x)$ for which $y_1'(x)=y_1(x)$ and $y_2(x)$. Then consider the new function $f(x) = ay_1(x)+by_2(x)$, where $a,b\\in\\mathbb{R}$ are scalars. Then\n",
    "$$f'(x) = ay_1'(x)+by_2'(x) = ay_1(x)+by_2(x) = f(x)$$\n",
    "\n",
    "c) This is a vector space. Consider two vectors $x_1$ and $x_2$ for which $Ax_1=0, Bx_1=0, Ax_2 = 0$ and $Bx_2 = 0$. Then $A(ax_1+bx_2) = aAx_1+bAx_2 =0$ and $B(ax_1+bx_2) = aBx_1+bBx_2 = 0$.\n",
    "\n",
    "d) This is generally **not** a vector space. Consider a vector $x_1$ for which $Ax_1 = 0$ but $Bx_1\\neq0$, and another vector $x_2$ for which $Bx_2=0$ but $Ax_2\\neq 0$. Then\n",
    "\\begin{align*}\n",
    "A(ax_1+bx_2)= aAx_1 + bAx_2 = bAx_2 \\neq 0 \\\\\n",
    "B(ax_1+bx_2)= aBx_1 + bBx_2 = aBx_1 \\neq 0\n",
    "\\end{align*}\n",
    "and so arbitrary linear combinations do not satisfy either condition.\n",
    "\n",
    "e) This is a vector space. Consider two function $f(x)$ and $g(x)$ for which $f(2019)=g(2019)=0$. Consider the new function $h(x) = af(x)+bg(x)$. Then $h(2019) = af(2019)+bg(2019) = 0$, and so $h(x)$ is also in the space.\n",
    "\n",
    "f) This is a vector space. Consider two $10\\times 10$ matrices $A$ and $B$, with components $A_{ij}$ and $B_{ij}$ for which $\\sum_{i=1}^{10} A_{ii} = \\sum_{i=1}^{10} B_{ii} = 0$. Consider the new matrix $C=aA+bB$ which has components $C_{ij} = aA_{ij}+bB_{ij}$. Then \n",
    "$$\\sum_{i=1}^{10} C_{ii} = \\sum_{i=1}^{10} (aA_{ii}+bB_{ii}) = a \\sum_{i=1}^{10} A_{ii} + b\\sum_{i=1}^{10} B_{ii} = 0$$\n",
    "so $C$ is also in the space. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5.\n",
    "\n",
    "Factor $\\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}$ into an upper triangular $U$ times a unit lower triangular $L$.  When is this impossible?  Write the three parameters in $U$ and the one parameter in $L$ in terms of $a,b,c,d$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "We saw this on pset 1. We have $$A = \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ x & 1 \\end{pmatrix}\\begin{pmatrix} u & v \\\\ 0 & w \\end{pmatrix}$$\n",
    "Multiplying out the two matrices on the right hand yields:\n",
    "$$\\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix} = \\begin{pmatrix} u & v \\\\ ux & vx+w\\end{pmatrix}$$\n",
    "This gives us four equations:\n",
    "\\begin{align*}\n",
    "u &= a\\\\\n",
    "v &= b\\\\\n",
    "ux &= c\\\\\n",
    "vx+w &= d.\n",
    "\\end{align*}\n",
    "The first two equations immediately tell us that $u=a$ and $v=b$. We can solve the third to tell us that $x = c/a$, and then substituting into the fourth tells us that $w = d-bc/a$. We have thus found the LU factorization:\n",
    "$$\\boxed{A = \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ c/a & 1 \\end{pmatrix}\\begin{pmatrix} a & b \\\\ 0 & d-bc/a \\end{pmatrix}}$$\n",
    "\n",
    "This exists provided that $\\boxed{a\\neq 0}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 6.\n",
    "\n",
    "Suppose a rank-1 matrix is written in terms of the rank r svd, as $A=uσv^T$, where $\\sigma>0$ is a scalar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) What is the sum of squares of the entries in the first column of $A$ in terms of entries in $u$,$v$ and $\\sigma$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) In simplest form, what is the sum of squares of all the entries of $A$ in terms of possibly $u$,$v$, and $\\sigma?$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "Suppose $A$ is a $m\\times n$ matrix, with $A = u\\sigma v^T$. Then $A$ has components $A_{ij} = \\sigma u_iv_j$. \n",
    "a) The sum of squares of the entries in the first column of $A$ is then\n",
    "$$\\sum_{i=1}^m A_{i1}^2 = \\sigma^2v_1^2\\sum_{i=1}^m u_i^2 = \\sigma^2v_1^2\\Vert u \\Vert^2$$\n",
    "\n",
    "b) The sum of squares of all the entries of $A$ is\n",
    "$$\\sum_{j=1}^n \\sum_{i=1}^m A_{ij}^2 = \\sigma^2\\Vert u \\Vert^2 \\sum_{j=1}^n v_j^2 = \\sigma^2\\Vert u \\Vert^2\\Vert v \\Vert^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 7\n",
    "\n",
    "Suppose $U = [U_1 | U_2]$ is a square $U$ matrix from the full SVD of an mxn A, what is $U_1^T U_2$? \n",
    "\n",
    "Pick from one of the four multiple choices that follow and explain your answer. <br>\n",
    "a) The rxr identity.  b) The rxr zero matrix. c) the mxm identity. d)the mxm zero matrix. e) none of the choices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "If $U$ is a square matrix, then we saw on pset 2 that it must be $m\\times m$. $U_1$ will then be $m\\times r$, and $U_2$ will be $m\\times (m-r)$. Since the columns of $U_1$ and $U_2$ are all mutually orthogonal, $U_1^TU_2$ will be a zero matrix. It will have dimensions $r\\times (m-r)$. So the correct answer is e)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 8.\n",
    "\n",
    "The computation below computes the svd of a random 5x5 matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×5 Array{Float64,2}:\n",
       " -0.54508    0.372748    0.441066  -0.406584   -0.45177  \n",
       " -0.383209  -0.18862    -0.669183   0.23708    -0.559965 \n",
       " -0.454786   0.352484    0.150352   0.735638    0.324278 \n",
       " -0.462471   0.0358319  -0.425429  -0.484084    0.607873 \n",
       " -0.367871  -0.836632    0.392504   0.0546038   0.0876234"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "5-element Array{Float64,1}:\n",
       " 2.4561346182560286 \n",
       " 0.9480288634718752 \n",
       " 0.5286559187859439 \n",
       " 0.2551801492474602 \n",
       " 0.04315896131157771"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "5×5 Adjoint{Float64,Array{Float64,2}}:\n",
       " -0.379804  -0.236369   -0.804333   0.0485018  -0.388039 \n",
       " -0.216729  -0.782325    0.323154   0.479952    0.0788237\n",
       " -0.619317  -0.0314431   0.429187  -0.564884   -0.334906 \n",
       " -0.427201  -0.0108674  -0.218996  -0.203307    0.853281 \n",
       " -0.49267    0.575318    0.12829    0.63786    -0.0544258"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using LinearAlgebra\n",
    "A = rand(5,5)\n",
    "U,s,V = svd(A)\n",
    "display(U)\n",
    "display(s)\n",
    "display(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Circle the numbers in the above that would figure in the best rank 3 approximation of $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "In the best rank 3 approximation to $A$, we would use the first three columns of both $U$ and $V$, in addition to the first three singular values in $s$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 9.\n",
    "\n",
    "If you multiply through the block matrices $\\Sigma$ and $V^T$ that appear in the full SVD as below,  <br>\n",
    "$\\begin{pmatrix} \\Sigma_r & 0 \\\\ 0 & 0 \\end{pmatrix}\n",
    "\\begin{pmatrix} V_1^T \\\\ V_2^T \\end{pmatrix}$\n",
    "the\n",
    "answer is (pick your multiple choice which is the best answer and briefly explain): <br>\n",
    "a) $\\Sigma_r V_1^T $ <br>\n",
    "b)$\\ [\\Sigma_r V_1^T \\ \\ 0]$ <br>\n",
    "c) $\\begin{pmatrix}\\Sigma_r V_1^T  \\\\ 0 \\end{pmatrix}$ <br>\n",
    "d) $\\begin{pmatrix} \\Sigma_r  V_1^T & 0 \\\\ 0 & 0 \\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "The correct answer must have the same number of rows as $\\begin{pmatrix} \\Sigma_r & 0 \\\\ 0 & 0 \\end{pmatrix}$, and the same number of columns as $\\begin{pmatrix} V_1^T \\\\ V_2^T \\end{pmatrix}$. The only option for which this is true is c). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 10.\n",
    "\n",
    "Suppose $v$ and $w$ are perpendicular in $R^n$.  What is $\\|v+w\\|^2$ in terms of $\\|v\\|^2$ and $\\|w\\|^2$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "If $v$ and $w$ are perpendicular, then $v^Tw = w^T v=0$. Then\n",
    "\\begin{align*}\n",
    "\\Vert v +w \\Vert^2 &= (v+w)^T(v+w)\\\\\n",
    "&= v^Tv+w^Tv+v^Tw+w^Tw\\\\\n",
    "&= v^Tv+w^Tw\\\\\n",
    "&= \\Vert v\\Vert^2 + \\Vert w \\Vert^2.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 11.\n",
    "\n",
    "Set up a matrix least squares problem if we are interested in taking $n$ data points $(x_i,y_i)$ and we wish to find the best function $f(x)=c_1 e^x + c_2 e^{-x}$ through the data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "We wish to find the constants $c_1$ and $c_2$ that satisfy:\n",
    "\\begin{align*}\n",
    "y_1 &= c_1 e^{x_1}+ c_2e^{-x_1}\\\\\n",
    "y_2 &= c_1 e^{x_2}+ c_2e^{-x_2}\\\\\n",
    "&...\\\\\n",
    "y_n &= c_1 e^{x_n}+ c_2e^{-x_n}\n",
    "\\end{align*}\n",
    "This is equivalent to a linear system $Ax= b$, where\n",
    "$$A = \\begin{pmatrix} e^{x_1} & e^{-x_1} \\\\ e^{x_2} & e^{-x_2} \\\\ \\vdots & \\vdots \\\\ e^{x_n} & e^{-x_n}\\end{pmatrix}, \\;\\; x = \\begin{pmatrix} c_1 \\\\ c_2 \\end{pmatrix} \\;\\;\\; \\text{and} \\;\\;\\; b = \\begin{pmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{pmatrix}$$\n",
    "In general, there will not be an exact solution for $x$. However, we can use $QR$ to find a least squares solution $\\hat{x}$ which minimizes the least squares error $\\Vert Ax-b \\Vert^2$. If $A=QR$, then this least squares solution is given by \n",
    "$$\\hat{x} = R^{-1}Q^T b.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 12\n",
    "\n",
    "Suppose a square $A$ has an LU factorization $A=LU$ where $L$ and $U$ are invertible.  If $A=QR$, what is $r_{11}$ in terms of possibly elements of $L$ and $U$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution \n",
    "\n",
    "If $A=LU$, then the first column of $A$ (call this vector $a_1$) is the first column of $L$ (call this vector $l_1$), multiplied by the upper left element of $U$ (call this element $u_{11}$), i.e.\n",
    "$$a_1 = u_{11}l_1.$$\n",
    "\n",
    "Now $r_{11}$ (The upper left element of $R$) is the magnitude of the first column of $A$ (since all the columns of $Q$ are normalized), and so\n",
    "$$r_{11} = \\Vert a_1 \\Vert = \\vert u_{11}\\vert \\Vert l_1 \\Vert$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.3",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
