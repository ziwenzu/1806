\documentclass[11pt]{article}
\usepackage[hmargin=40pt,vmargin=40pt]{geometry}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{enumerate}
\pagenumbering{gobble} 
\newcommand{\diff}{\,\mathrm{d}}
\renewcommand*{\vec}[1]{\mathbf{#1}}

\title{18.06 R08 - Recitation 2 - SOLUTIONS}
\author{Sam Turton}
\date{February 26, 2019}                                      
\begin{document}
\maketitle

\section{Practice problems for midterm 1}

\noindent \textbf{Problem 1. (LU factorization)}\\
\begin{enumerate}[(a)]
\item Compute the $LU$ factorization of an arbitrary $2\times 2$ matrix, i.e. find $x, u,v$ and $w$ for which
$$\begin{pmatrix} a & b \\ c & d \end{pmatrix} = \begin{pmatrix} 1 & 0 \\ x & 1 \end{pmatrix}\begin{pmatrix} u & v \\ 0 & w \end{pmatrix}.$$
Are there any conditions on $a,b,c$ and/or $d$ for this factorization to exist?
\item Find an $LU$ factorization for the following $4\times 4$ matrix:
$$A = \begin{pmatrix} 1 & 2 & 0 & 0 \\ 2 & 3 & 0 & 0 \\ 0 & 0 & 1 & -1 \\ 0 & 0 & -2 & 5 \end{pmatrix}$$
\end{enumerate}

\vskip 10pt

\noindent \textbf{Solution}\\
\begin{enumerate}[(a)]
\item We did this on pset 1. We found that
\begin{align*}
u & = a \\
v & = b \\
x &= c/a \\ 
w &= d-bc/a
\end{align*}
The only condition for this factorization to exist is that $a\neq 0 $. 
\item Although we have only found the $LU$ factorization for $2\times 2$ matrices, we can spot that this $4\times 4$ matrix is a \emph{block} matrix, and the two blocks are both $2\times 2$ matrices. We can therefore write 
$$A = \begin{pmatrix} 1 & 2 & 0 & 0 \\ 2 & 3 & 0 & 0 \\ 0 & 0 & 1 & -1 \\ 0 & 0 & -2 & 5 \end{pmatrix} 
       = \begin{pmatrix} A_1 & 0 \\ 0 & A_2 \end{pmatrix}
       = \begin{pmatrix} L_1U_1 & 0 \\ 0 & L_2U_2 \end{pmatrix}
       = \begin{pmatrix} L_1 & 0 \\ 0 & L_2 \end{pmatrix} \begin{pmatrix} U_1 & 0 \\ 0 & U_2 \end{pmatrix}
       = LU$$
where, e.g., $L_1U_1$ is the $LU$ factorization of $A_1$. We then find that
$$L = \begin{pmatrix} 1 & 0 & 0 & 0\\2 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0\\ 0 & 0 & -2 & 1 \end{pmatrix} \;\;\;
   U = \begin{pmatrix} 1 & 2 & 0 & 0\\0 & -1 & 0 & 0\\ 0 & 0 & 1 & -1 \\ 0 & 0 & 0 & 3 \end{pmatrix}$$
\end{enumerate}

\newpage

\noindent \textbf{Problem 2. (Vector spaces)}\\
Are the following sets examples of vector spaces? If yes, then show that an arbitrary linear combination of two elements in the set is also in the set. If not, then explain why.
\begin{enumerate}[(a)]
\item The set of all solutions, $x$, to the equation $Ax = \begin{pmatrix} 1 \\ 2 \\ 3 \end{pmatrix}$; where $x\in\mathbb{R}^5$ and $A$ is a $3\times 5$ matrix. 
\item The set of all $3\times 2$ matrices $X$ for which $AX = 0$, where $A$ is a fixed $5\times 3$ matrix. 
\item The set of all $2\times 2$ matrices $A$ for which $A^{-1}$ does \emph{not} exist. 
\item The set of all differentiable functions $f(x)$ for which $f'(0) = 2f(0)$. 
\item The set of all functions $f(x)$ for which $f(x+y)=f(x)f(y)$.\footnote{Aside for those interested: what continuous functions obey this rule?} 
\end{enumerate}

\vskip 10pt

\noindent \textbf{Solution}\\
\begin{enumerate}[(a)]
\item This is not a vector space. It does not contain the zero vector, because $x=0$ does not satisfy the equation.
\item This is a vector space. Take two matrices $X_1$ and $X_2$ for which $AX_1=AX_2 = 0$, and two scalars $a,b\in\mathbb{R}$. Then $A(aX_1+bX_2) = aAX_1+bAX_2 = 0$. 
\item This is not a vector space. It does contain the zero matrix. However, $X_1 = \begin{pmatrix} 1 & 0 \\ 0 & 0 \end{pmatrix}$ and $X_2=\begin{pmatrix} 0 & 0 \\ 0 & 1 \end{pmatrix}$ are not invertible matrices (check $ad-bc = 0$), but $X_1+X_2 = I$, which is invertible!
\item This is a vector space. Consider two functions $f_1(x)$ and $f_2(x)$ for which $f_1'(0)=2f_1(0)$ and $f_2'(0)=2f_2(0)$, and two scalars $a,b\in\mathbb{R}$. A linear cominbation of $f_1$ and $f_2$ is a new function $g(x) = af_1(x)+bf_2(x)$. Then $g'(0)= af_1'(0)+bf_2'(0) = 2af_1(0)+2bf_2(0) = 2g(0)$. 
\item This is not a vector space. If $f_1(x)$ is a function for which $f_1(x+y)=f(x)f(y)$, then $af_1(x+y)=af_1(x)f_1(y)\neq(af_1(x))(af_2(x))$ for all $a\neq1$. 
\end{enumerate}

\vskip 10pt

\noindent \textbf{Problem 3. (QR factorization)}\\
Consider the following $3\times 2$ matrix $A$:
$$A = \begin{pmatrix} 1 & 0 \\ 1 & 1 \\ -1 & 1 \end{pmatrix}$$
\begin{enumerate}[(a)]
\item Write $A=QR$, where $Q$ is an orthogonal matrix ($Q^TQ=I$) and $R$ is a square, upper triangular matrix. What is $R^{-1}$? Is $Q$ invertible? 
\item Consider the linear system
$$Ax = \begin{pmatrix} 1 \\ 1 \\ 1 \end{pmatrix}.$$
Can we express the right hand side of this linear system as a linear combination of the columns of $A$?
\item Check that $QQ^T\begin{pmatrix} 1 \\ 1 \\ 1 \end{pmatrix} \neq \begin{pmatrix} 1 \\ 1 \\ 1 \end{pmatrix}$. 
\item Find explicitly the vector $\hat{x}$ that minimizes $\Vert Ax - b \Vert^2$. 
\end{enumerate}

\vskip 10pt

\noindent \textbf{Solution}\\
\begin{enumerate}[(a)]
\item The columns of $A$ are already orthogonal. However, they are not normalized. We can then write $A=QR$, where 
$$Q = \begin{pmatrix} 1/\sqrt{3} & 0 \\ 1/\sqrt{3} & 1/\sqrt{2} \\ -1/\sqrt{3} & 1/\sqrt{2} \end{pmatrix} \;\;\; R = \begin{pmatrix} \sqrt{3} & 0 \\ 0 & \sqrt{2} \end{pmatrix}.$$
See lecture slide 55 for more details. $R$ is a diagonal $2\times 2$, so we can then compute as $R^{-1} =  \begin{pmatrix} 1/\sqrt{3} & 0 \\ 0 & 1/\sqrt{2} \end{pmatrix}$. $Q$ is a rectangular matrix, so it cannot be invertible. 
\item This equation does not have any solutions, because there is no linear combination of the columns of $A$ which will give the RHS
\item $$QQ^T \begin{pmatrix} 1 \\ 1 \\ 1 \end{pmatrix} = \begin{pmatrix} 1/3 \\ 4/3 \\ 2/3 \end{pmatrix}$$
\item Recall that the least squares solution $\hat{x}$ obeys $\hat{x} = R^{-1}Q^T\begin{pmatrix} 1 \\ 1 \\ 1 \end{pmatrix} $. We have $R^{-1}$ and $Q$ so we can calculate explicitly that $\hat{x} = \begin{pmatrix} 1/3 \\ 1 \end{pmatrix}$. 
\end{enumerate}

\newpage

\noindent \textbf{Problem 4. (SVD)}\\
Consider the $3\times 3 $ matrix $A$ with the following full SVD:
$$A  = \begin{pmatrix} 1/\sqrt{2} &  1/\sqrt{2} & 0 \\  1/\sqrt{2} & - 1/\sqrt{2} & 0 \\ 0 & 0 & 1 \end{pmatrix} \begin{pmatrix} 10 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 0 \end{pmatrix} \begin{pmatrix} 0 & 0 & 1 \\ 1 & 0 & 0 \\ 0 & 1  & 0 \end{pmatrix}^T$$
\begin{enumerate}[(a)]
\item Describe each of the four fundamental subspaces of $A$ (column space, row space, nullspace, and left nullspace)
\item Find the best rank-1 approximation to $A$. Write your answer as a $3\times 3$ matrix.
\item Does $Ax = \begin{pmatrix} 1 \\ 2 \\ 3 \end{pmatrix}$ have a solution? 
\item Does $Ax = \begin{pmatrix} 1 \\ 2 \\ 0 \end{pmatrix}$ have a solution? 
\end{enumerate}

\vskip 10pt

\noindent \textbf{Solution}\\
\begin{enumerate}[(a)]
\item 
\begin{itemize}
\item The column space of $A$, $C(A)$, is equal to the column space of $U_1$. This is all linear combinations of $\begin{pmatrix} 1/\sqrt{2} \\  1/\sqrt{2} \\ 0\end{pmatrix}$ and $\begin{pmatrix} 1/\sqrt{2} \\ -1/\sqrt{2} \\ 0\end{pmatrix}$
\item The row space of $A$, $C(A^T)$, is equal to the column space of $V_1$. This is all linear combinations of $\begin{pmatrix} 0 \\ 1 \\ 0\end{pmatrix}$ and $\begin{pmatrix} 0 \\ 0 \\ 1\end{pmatrix}$
\item The nullspace of $A$, $N(A)$, is equal to the column space of $V_2$. This is all linear combinations of $\begin{pmatrix} 1 \\ 0 \\ 0\end{pmatrix}$.
\item The left nullspace of $A$, $N(A^T)$, is equal to the column space of $U_2$. This is all linear combinations of $\begin{pmatrix} 0 \\ 0 \\ 1 \end{pmatrix}$.
\end{itemize}
\item The best rank-1 approximation of $A$ is
$$\sigma_1 u_1 v_1^T = 10 \begin{pmatrix} 1/\sqrt{2} \\  1/\sqrt{2} \\ 0\end{pmatrix} \begin{pmatrix} 0 & 1 & 0 \end{pmatrix} = 10 \begin{pmatrix} 0 & 1/\sqrt{2} & 0 \\ 0 & 1/\sqrt{2} & 0 \\ 0 & 0 & 0 \end{pmatrix}$$
\item We can compute $U_1U_1^T \begin{pmatrix} 1 \\ 2 \\ 3 \end{pmatrix} =  \begin{pmatrix} 1 \\ 2 \\ 0 \end{pmatrix}$; and so the system does not have a solution (see lecture slide 68)
\item We can compute $U_1U_1^T \begin{pmatrix} 1 \\ 2 \\ 0 \end{pmatrix} =  \begin{pmatrix} 1 \\ 2 \\ 0 \end{pmatrix}$; and so the system does have a solution.
\end{enumerate}

\newpage

\noindent \textbf{Problem 5. (Vector algebra)}\\
Consider two vectors $u$ and $v$. Find a new vector $w$ which is in the direction of $v$, and for which $w-u$ is orthogonal to $v$. Your answer should be given in terms of the vectors $u$ and $v$.\footnote{Your answer $w$ is known as the \emph{projection} of $u$ onto $v$. More on this in class tomorrow}

\vskip 10pt

\noindent \textbf{Solution}\\
If $w$ is in the direction of $v$, then $w=\alpha v$ where $\alpha\in\mathbb{R}$. If $w-u$ is orthogonal to $v$, then 
$$0 = v^T(w-u) = v^T(\alpha v - u) = \alpha v^Tv - v^Tu$$
$$\implies \boxed{\alpha = \frac{v^T u}{v^T v}}$$
So that $\boxed{w = \frac{v^T u}{v^T v} v}$. 

\vskip 10pt 

\noindent \textbf{Problem 6. (Least squares)}\\
Suppose we are conducting an experiment and obtain the following $m$ data points $\{(a_1,b_1), (a_2,b_2), ..., (a_m,b_m)\}$. We think that our data is close to lying on a curve of the form $f(a) = x_1 + x_2 a + x_3 (a-1)^2$. How would we obtain the best fit coefficients $\hat{x}_1, \hat{x}_2$ and $\hat{x}_3$? What quantity does the best fit vector $\hat{x} = \begin{pmatrix} \hat{x}_1 \\ \hat{x}_2 \\ \hat{x}_3 \end{pmatrix}$ minimize? 

\vskip 10pt

\noindent \textbf{Solution}\\
If a perfect fit existed, then we would want $\hat{x}$ to satisfy the equation:
\begin{align*}
\begin{pmatrix} 1 & a_1 & (a_1-1)^2 \\ 1 & a_2 & (a_2-1)^2 \\ \vdots & \vdots & \vdots \\ 1 & a_m & (a_m-1)^2 \end{pmatrix} \begin{pmatrix} \hat{x}_1 \\ \hat{x}_2 \\ \hat{x}_3 \end{pmatrix} &= \begin{pmatrix} b_1 \\ b_2 \\ \vdots \\ b_m \end{pmatrix}\\
\iff A\hat{x} &= b
\end{align*}
However, this system of equations will generally not have a solution for $\hat{x}$. Instead, we seek $\hat{x}$ which minimizes the least squares error $\Vert Ax - b \Vert^2$. If we can find a QR factorization $A=QR$, then this least squares solution will obey
$$\boxed{\hat{x} = R^{-1}Q^T b}$$


\end{document}  