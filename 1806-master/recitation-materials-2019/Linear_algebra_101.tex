\documentclass[11pt]{article}
\usepackage[hmargin=60pt,vmargin=60pt]{geometry}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{enumerate}
\pagenumbering{gobble} 
\newcommand{\diff}{\,\mathrm{d}}
\renewcommand*{\vec}[1]{\mathbf{#1}}

\title{18.06 - Linear Algebra Cheatsheet}
\author{}
\date{February 22, 2019}                                      
\begin{document}
\maketitle


\rule{\textwidth}{1pt}
\vskip 10pt
\noindent \emph{Remark: This is \textbf{not} intended as a definitive list of everything you are meant to memorize for 18.06. Some of these ideas will be familiar, some of them less so. This is a reference document for you to look up definitions if you come across something that seems unfamiliar/confusing.}
\section{Vectors}
\begin{enumerate}
\item When we talk about \emph{vectors} in 18.06, we are usually referring to column vectors. 
\item A two-dimensional vector $\vec{v}$ is defined by its two components, $v_1$ and $v_2$. We write the vector as
$$\boxed{\vec{v} = \begin{pmatrix} v_1 \\ v_2 \end{pmatrix}}$$
\item The set of all two-dimensional vectors is referred to as $\mathbb{R}^2$
\item In general, a vector $\vec{v}$ can have $n$ components, and would then be an $n$-dimensional vector (a $n\times 1$ array):
$$\boxed{\vec{v} = \begin{pmatrix} v_1 \\ v_2 \\ \vdots \\ v_n \end{pmatrix}}$$
The set of all $n$-dimensional vectors is referred to as $\mathbb{R}^n$.
\item We can always multiply vectors by scalars. We can also add two vectors, provided they have the same dimensions.
\item The zero, $n$-dimensional vector $\vec{0}$ is a vector where every component is $0$. 
\item The \emph{length} (or \emph{magnitude}) of a vector $\vec{v}$ is written $\Vert \vec{v} \Vert$. It is given by the following formula:
$$\boxed{\Vert \vec{v} \Vert^2 = v_1^2 + v_2^2 +...+ v_n^2 =  \sum_{i=1}^n v_i^2}$$
\item A \emph{unit vector} $\vec{n}$ is a vector with length $\Vert\vec{n}\Vert =1$. 
\item Suppose you have two $n$-dimensional vectors, $\vec{u}$ and $\vec{v}$:
$$\vec{u} = \begin{pmatrix} u_1 \\ u_2 \\ \vdots \\ u_n \end{pmatrix}, \;\; \vec{v} = \begin{pmatrix} v_1 \\ v_2 \\ \vdots \\ v_n \end{pmatrix}$$
Then the \emph{dot product} (or \emph{inner product}) of these two vectors, $\vec{u}\cdot\vec{v}$, is given by the following formula:
$$\boxed{\vec{u}\cdot\vec{v} = u_1v_1 + u_2v_2 + ... + u_nv_n}$$
\item The \emph{angle} between two vectors $\vec{u}$ and $\vec{v}$ is given by the following formula:
$$\boxed{\cos{\theta} = \frac{\vec{u}\cdot\vec{v}}{\Vert \vec{u}\Vert\Vert\vec{v}\Vert}}$$
\item We say that a nonzero vector $\vec{u}$ is \emph{parallel} to a nonzero vector $\vec{v}$ if $\boxed{\vec{u} = a\vec{v}}$ for some scalar $a\neq 0$. We sometimes say that $\vec{u}$ and $\vec{v}$ are in the same direction.
\item We say that a vector $\vec{u}$ is \emph{perpendicular}, or \emph{orthogonal}, to a vector $\vec{v}$ if $\boxed{\vec{u}\cdot\vec{v} = 0}$. 
\item The \emph{transpose} of a vector is an $n$-dimensional row vector (a $1\times n$ array):
$$\boxed{\vec{v}^T = \begin{pmatrix} v_1 & v_2 & \cdots & v_n \end{pmatrix}}$$
\item We can multiply a $n$-dimensional row vector by a $n$-dimensional column vector. The order of multiplication matters:
\begin{itemize}
\item $\vec{u}^T \vec{v}$ has dimensions $1\times 1$, i.e. it is a scalar. In fact $\vec{u}^T\vec{v} = \vec{v}^T\vec{u}$:
$$\vec{u}^T \vec{v} = \begin{pmatrix} u_1 & u_2 & \cdots & u_n \end{pmatrix}  \begin{pmatrix} v_1 \\ v_2 \\ \vdots \\ v_n \end{pmatrix} = u_1v_1 + u_2v_2 + ... + u_nv_n$$
We see then that really $\vec{u}^T\vec{v} = \vec{u}\cdot\vec{v}$, i.e. the \emph{dot product} of $\vec{u}$ and $\vec{v}$. 
\item $\vec{u} \vec{v}^T$ has dimensions $n\times n$. It is called an \emph{outer product}. 
\end{itemize}
\end{enumerate}

\newpage

\section{Matrices}
\subsection{General properties}
\paragraph{Remark.} The most basic rule that you should remember:
\framebox{\textbf{row-column}}. It shows the order in which you write or compute, e.g.:
\begin{itemize}
    \item The first index denotes the row, the second number the column.
    \item You multiply a row by a column to get a number.
    \item An $m \times n$ matrix has $m$ rows and $n$ columns.
\end{itemize}
\vskip 30pt
\begin{enumerate}
\item A \emph{matrix} is an $m \times n$ array of numbers. An $m\times n$ matrix has $m$ \emph{rows} and $n$ \emph{columns}. A matrix is \emph{square} if $m=n$.  Examples:
\begin{itemize}
\item $A = \begin{pmatrix} 1 & 2 \\ -1 & 3 \end{pmatrix}$ is a $2\times 2$ square matrix.
\item $B = \begin{pmatrix} 1 & 0 & 1 \\ -1 & 2 & 3 \end{pmatrix}$ is a $2\times 3$ matrix.
\item $C = \begin{pmatrix} 1 & 4  \\ 0 & -3  \\ 1 & 1 \end{pmatrix}$ is a $3\times 2$ matrix. 
\end{itemize}
\item Suppose $A$ is a $m\times n$ matrix and $B$ is a $p\times q$ matrix. We can only multiply these matrices if the dimensions make sense. We can multiply $AB$ only if $n=p$; we can multiply $BA$ only if $m=q$.
\item Suppose $A$ and $B$ are two $n\times n$ square matrices. In general $\boxed{AB \neq BA}$. Matrix multiplication does not \emph{commute}. 
\item A \emph{diagonal} matrix is a matrix which only has entries along its diagonal.
\item The \emph{identity} matrix $I$ is a square, diagonal matrix. The entries along the diagonal are all equal to 1. The identity matrix is the only matrix for which $IA=A$ for all square matrices $A$. 
\item We can compute the product of three matrices $ABC$ either as $(AB)C$ (multiply $AB$ and then multiply on the right by $C$), or as $A(BC)$ (multiply $BC$ and then multiply on the left by $A$). Matrix multiplication is \emph{associative}. 
\item Matrix multiplication is \emph{distributive}. This means that $(A+B)C = AC + BC$. 
\end{enumerate}
\subsection{Linear systems}
\begin{enumerate}
\item Often we will be interested in solving equations of the form $\boxed{Ax=b}$, where $A$ is an $m\times n$ matrix, $\vec{x}$ is a $n$-dimensional vector, and $\vec{b}$ is a $m$-dimensional vector. This is usually called a \emph{linear system}.
\item If $A$ is a square matrix, then it might have an \emph{inverse} $A^{-1}$, so that $AA^{-1}=A^{-1}A=I$. The  unique solution of the linear system $A\vec{x}=\vec{b}$ in this case is $\vec{x}=A^{-1}\vec{b}$. 
\item If $A$ and $B$ are square matrices with inverses $A^{-1}$ and $B^{-1}$, then $\boxed{(AB)^{-1}=B^{-1}A^{-1}}$. 
\item The inverse of a square matrix does not always exist. We have already seen that for a $2\times 2$ matrix $A=\begin{pmatrix} a & b \\ c & d \end{pmatrix}$, we have the following formula
$$\boxed{A^{-1} = \frac{1}{ad-bc} \begin{pmatrix} d & -b \\ -c & a \end{pmatrix}}$$
This exists if and only if $\boxed{ad-bc\neq 0}$. 
\item Rectangular matrices for which $m\neq n$ will not have inverses.
\item A general linear system $Ax=b$, where $A$ is a rectangular $m\times n$ matrix, \emph{may} have a unique solution. It may also have infinitely many solutions, or no solutions at all. 
\end{enumerate}
\subsection{Transposes}
\begin{enumerate}
\item The \emph{transpose} of a matrix $A$ is denoted by $A^T$. The transpose is the matrix formed by taking the columns of $A$ and making them the rows of $A^T$. 
\item If $A$ has components $a_{ij}$, then $A^T$ has components $a_{ji}$.
\item If $A$ is $m\times n$, then $A^T$ is $n\times m$.
\item $A^TA$ and $AA^T$ can always be computed. They are both square matrices, even if $A$ is rectangular. If $A$ is $m\times n$, then $A^TA$ is $m\times m$ and $AA^T$ is $n\times n$.
\item $\boxed{(AB)^T = B^TA^T}$.
\item If $A$ is a square matrix and $A^{-1}$ exists, then $\boxed{(A^T)^{-1} = (A^{-1})^T}$. 
\end{enumerate}
\subsection{Orthogonal matrices}
\begin{enumerate}
\item A $n\times n$ square matrix $Q$ is \emph{orthogonal} if $\boxed{Q^TQ=I}$. A rectangular matrix for which $Q^TQ=I$ we will usually refer to as a \emph{tall skinny orthogonal} matrix. 
\item An equivalent definition: suppose a matrix $Q$ has $n$ columns given by the vectors $\vec{q}_1, ..., \vec{q}_n$. Then $Q$ is orthogonal if the column vectors are orthonormal. This means that $q_i^Tq_j = 0$ for $i\neq j$, and $\Vert q_i \Vert = 1$. 
\item If $Q$ is square and orthogonal, then we also have that $QQ^T = I$, and so $Q^{-1}=Q^T$.
\item If $Q$ obey $Q^TQ=I$, but is not square, then $QQ^T \neq I$ generally! 
\end{enumerate}

\newpage

\section{Planes, hyperplanes and surfaces}
\subsection{Planes}
\begin{enumerate}
\item The general equation of a plane in $\mathbb{R}^3$ is 
$$\boxed{ax+by+cd = f,}$$
where $a,b,c,d\in\mathbb{R}$ are scalars. The plane contains the origin if and only if $f=0$.
\item The normal to this plane is given by the vector $\vec{w} = \begin{pmatrix} a \\ b \\ c\end{pmatrix}$.
\item The equation of the plane may equivalently be written as $\vec{w}\cdot\vec{x} = \vec{w}^T\vec{x} = f$, where $\vec{x} = \begin{pmatrix} x \\ y \\ z \end{pmatrix}$.
\end{enumerate}
\subsection{Hyperplanes}
\begin{enumerate}
\item A hyperplane in $\mathbb{R}^n$ is the set of points with position vectors $\vec{x} = \begin{pmatrix} x_1\\ \vdots\\x_n\end{pmatrix}$ obeying the equation
$$\boxed{w_1x_1+w_2x_2 + ... w_nx_n = f}$$
for scalars $w_1,w_2, ..., w_n\in \mathbb{R}$.
\item  The normal to this plane is given by the vector $\vec{w} = \begin{pmatrix} w_1\\ \vdots \\ w_n\end{pmatrix}$.
\item The equation of the hyperplane may equivalently be written as $\vec{w}\cdot\vec{x} = \vec{w}^T\vec{x} = f$.
\end{enumerate}
\subsection{Surfaces}
\begin{itemize}
\item A surface in three dimensions is described by an equation $f(x,y,z)=Const.$
\item The \emph{normal} to a surface is $\boxed{\vec{n} = \nabla f}$.
\end{itemize}








\end{document}  